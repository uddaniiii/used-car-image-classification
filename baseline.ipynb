{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE': 224,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'EPOCHS': 100,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SEED' : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed ê³ ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "\n",
    "        if is_test:\n",
    "            # í…ŒìŠ¤íŠ¸ì…‹: ë¼ë²¨ ì—†ì´ ì´ë¯¸ì§€ ê²½ë¡œë§Œ ì €ì¥\n",
    "            for fname in sorted(os.listdir(root_dir)):\n",
    "                if fname.lower().endswith(('.jpg')):\n",
    "                    img_path = os.path.join(root_dir, fname)\n",
    "                    self.samples.append((img_path,))\n",
    "        else:\n",
    "            # í•™ìŠµì…‹: í´ë˜ìŠ¤ë³„ í´ë” êµ¬ì¡°ì—ì„œ ë¼ë²¨ ì¶”ì¶œ\n",
    "            self.classes = sorted(os.listdir(root_dir))\n",
    "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "            for cls_name in self.classes:\n",
    "                cls_folder = os.path.join(root_dir, cls_name)\n",
    "                for fname in os.listdir(cls_folder):\n",
    "                    if fname.lower().endswith(('.jpg')):\n",
    "                        img_path = os.path.join(cls_folder, fname)\n",
    "                        label = self.class_to_idx[cls_name]\n",
    "                        self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path = self.samples[idx][0]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = './data/train'\n",
    "test_root = './data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ì´ë¯¸ì§€ ìˆ˜: 33137\n",
      "train ì´ë¯¸ì§€ ìˆ˜: 26509, valid ì´ë¯¸ì§€ ìˆ˜: 6628\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "full_dataset = CustomImageDataset(train_root, transform=None)\n",
    "print(f\"ì´ ì´ë¯¸ì§€ ìˆ˜: {len(full_dataset)}\")\n",
    "\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "# Stratified Split\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(targets)), test_size=0.2, stratify=targets, random_state=42\n",
    ")\n",
    "\n",
    "# Subset + transform ê°ê° ì ìš©\n",
    "train_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\n",
    "val_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\n",
    "print(f'train ì´ë¯¸ì§€ ìˆ˜: {len(train_dataset)}, valid ì´ë¯¸ì§€ ìˆ˜: {len(val_dataset)}')\n",
    "\n",
    "\n",
    "# DataLoader ì •ì˜\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)  # ResNet18 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "        self.feature_dim = self.backbone.fc.in_features \n",
    "        self.backbone.fc = nn.Identity()  # feature extractorë¡œë§Œ ì‚¬ìš©\n",
    "        self.head = nn.Linear(self.feature_dim, num_classes)  # ë¶„ë¥˜ê¸°\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)       \n",
    "        x = self.head(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/ Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wandb ì—°ë™ & model ì €ì¥ ê²½ë¡œ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline</strong> at: <a href='https://wandb.ai/danyou2260-boostcamp-tech/used-car-classification/runs/rh718rc2' target=\"_blank\">https://wandb.ai/danyou2260-boostcamp-tech/used-car-classification/runs/rh718rc2</a><br> View project at: <a href='https://wandb.ai/danyou2260-boostcamp-tech/used-car-classification' target=\"_blank\">https://wandb.ai/danyou2260-boostcamp-tech/used-car-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250521_115444-rh718rc2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\uddaniiii\\used-car-image-classification\\wandb\\run-20250521_115452-w3axcvjt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danyou2260-boostcamp-tech/used-car-classification/runs/w3axcvjt' target=\"_blank\">baseline</a></strong> to <a href='https://wandb.ai/danyou2260-boostcamp-tech/used-car-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danyou2260-boostcamp-tech/used-car-classification' target=\"_blank\">https://wandb.ai/danyou2260-boostcamp-tech/used-car-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danyou2260-boostcamp-tech/used-car-classification/runs/w3axcvjt' target=\"_blank\">https://wandb.ai/danyou2260-boostcamp-tech/used-car-classification/runs/w3axcvjt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"used-car-classification\",  # ë„ˆì˜ í”„ë¡œì íŠ¸ ì´ë¦„\n",
    "    name=f\"baseline\",  # ì‹¤í—˜ ì´ë¦„\n",
    "    config=CFG  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë¡\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"baseline\"  # ë„ˆê°€ ì„¤ì •í•œ ëª¨ë¸ ì´ë¦„\n",
    "SAVE_DIR = f\"./checkpoints/{MODEL_NAME}\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)  # ë””ë ‰í† ë¦¬ ì—†ìœ¼ë©´ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "[Epoch 1/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:05<00:00,  3.31it/s]\n",
      "[Epoch 1/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:29<00:00,  3.54it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 4.1384 || Valid Loss : 2.2268 | Valid Accuracy : 69.1460%\n",
      "ğŸ“¦ Best model saved at epoch 1 (./checkpoints/baseline\\1_4.1384_2.2268.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:04<00:00,  3.33it/s]\n",
      "[Epoch 2/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:29<00:00,  3.56it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.2644 || Valid Loss : 0.7774 | Valid Accuracy : 88.0507%\n",
      "ğŸ“¦ Best model saved at epoch 2 (./checkpoints/baseline\\2_1.2644_0.7774.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:05<00:00,  3.29it/s]\n",
      "[Epoch 3/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:29<00:00,  3.55it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.4117 || Valid Loss : 0.4506 | Valid Accuracy : 91.3247%\n",
      "ğŸ“¦ Best model saved at epoch 3 (./checkpoints/baseline\\3_0.4117_0.4506.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:09<00:00,  3.19it/s]\n",
      "[Epoch 4/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:29<00:00,  3.47it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.1659 || Valid Loss : 0.3504 | Valid Accuracy : 92.7429%\n",
      "ğŸ“¦ Best model saved at epoch 4 (./checkpoints/baseline\\4_0.1659_0.3504.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 5/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:04<00:00,  3.34it/s]\n",
      "[Epoch 5/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:29<00:00,  3.56it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0767 || Valid Loss : 0.3150 | Valid Accuracy : 92.9541%\n",
      "ğŸ“¦ Best model saved at epoch 5 (./checkpoints/baseline\\5_0.0767_0.3150.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 6/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:05<00:00,  3.32it/s]\n",
      "[Epoch 6/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:28<00:00,  3.67it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0426 || Valid Loss : 0.2842 | Valid Accuracy : 93.7236%\n",
      "ğŸ“¦ Best model saved at epoch 6 (./checkpoints/baseline\\6_0.0426_0.2842.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 7/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:03<00:00,  3.36it/s]\n",
      "[Epoch 7/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:28<00:00,  3.69it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0278 || Valid Loss : 0.2720 | Valid Accuracy : 93.7538%\n",
      "ğŸ“¦ Best model saved at epoch 7 (./checkpoints/baseline\\7_0.0278_0.2720.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 8/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:02<00:00,  3.38it/s]\n",
      "[Epoch 8/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:28<00:00,  3.65it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0204 || Valid Loss : 0.2567 | Valid Accuracy : 93.9952%\n",
      "ğŸ“¦ Best model saved at epoch 8 (./checkpoints/baseline\\8_0.0204_0.2567.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 9/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:05<00:00,  3.31it/s]\n",
      "[Epoch 9/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:29<00:00,  3.55it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0117 || Valid Loss : 0.2384 | Valid Accuracy : 94.1460%\n",
      "ğŸ“¦ Best model saved at epoch 9 (./checkpoints/baseline\\9_0.0117_0.2384.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 10/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:10<00:00,  3.19it/s]\n",
      "[Epoch 10/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:28<00:00,  3.61it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0131 || Valid Loss : 0.2340 | Valid Accuracy : 94.1611%\n",
      "ğŸ“¦ Best model saved at epoch 10 (./checkpoints/baseline\\10_0.0131_0.2340.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 11/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:03<00:00,  3.37it/s]\n",
      "[Epoch 11/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:28<00:00,  3.60it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0071 || Valid Loss : 0.2257 | Valid Accuracy : 94.4327%\n",
      "ğŸ“¦ Best model saved at epoch 11 (./checkpoints/baseline\\11_0.0071_0.2257.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 12/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:04<00:00,  3.32it/s]\n",
      "[Epoch 12/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:29<00:00,  3.47it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0036 || Valid Loss : 0.2240 | Valid Accuracy : 94.3724%\n",
      "ğŸ“¦ Best model saved at epoch 12 (./checkpoints/baseline\\12_0.0036_0.2240.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 13/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:04<00:00,  3.33it/s]\n",
      "[Epoch 13/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:29<00:00,  3.55it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.1042 || Valid Loss : 0.3524 | Valid Accuracy : 91.2492%\n",
      "ğŸ•’ No improvement in logloss for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 14/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:07<00:00,  3.26it/s]\n",
      "[Epoch 14/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:28<00:00,  3.68it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0325 || Valid Loss : 0.2667 | Valid Accuracy : 93.5124%\n",
      "ğŸ•’ No improvement in logloss for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 15/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:06<00:00,  3.28it/s]\n",
      "[Epoch 15/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:28<00:00,  3.68it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0133 || Valid Loss : 0.2229 | Valid Accuracy : 94.2366%\n",
      "ğŸ“¦ Best model saved at epoch 15 (./checkpoints/baseline\\15_0.0133_0.2229.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 16/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:04<00:00,  3.34it/s]\n",
      "[Epoch 16/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:28<00:00,  3.71it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0029 || Valid Loss : 0.2022 | Valid Accuracy : 94.7495%\n",
      "ğŸ“¦ Best model saved at epoch 16 (./checkpoints/baseline\\16_0.0029_0.2022.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 17/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:04<00:00,  3.33it/s]\n",
      "[Epoch 17/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:27<00:00,  3.76it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0018 || Valid Loss : 0.1984 | Valid Accuracy : 94.8099%\n",
      "ğŸ“¦ Best model saved at epoch 17 (./checkpoints/baseline\\17_0.0018_0.1984.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 18/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:05<00:00,  3.31it/s]\n",
      "[Epoch 18/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:27<00:00,  3.74it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0011 || Valid Loss : 0.1962 | Valid Accuracy : 94.6741%\n",
      "ğŸ“¦ Best model saved at epoch 18 (./checkpoints/baseline\\18_0.0011_0.1962.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 19/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:04<00:00,  3.33it/s]\n",
      "[Epoch 19/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:28<00:00,  3.59it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0009 || Valid Loss : 0.1949 | Valid Accuracy : 94.9306%\n",
      "ğŸ“¦ Best model saved at epoch 19 (./checkpoints/baseline\\19_0.0009_0.1949.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 20/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:07<00:00,  3.25it/s]\n",
      "[Epoch 20/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:29<00:00,  3.50it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0660 || Valid Loss : 0.3228 | Valid Accuracy : 91.7924%\n",
      "ğŸ•’ No improvement in logloss for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 21/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:06<00:00,  3.29it/s]\n",
      "[Epoch 21/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:27<00:00,  3.73it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0229 || Valid Loss : 0.2686 | Valid Accuracy : 93.0899%\n",
      "ğŸ•’ No improvement in logloss for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 22/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:04<00:00,  3.32it/s]\n",
      "[Epoch 22/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:29<00:00,  3.56it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0084 || Valid Loss : 0.2300 | Valid Accuracy : 94.0253%\n",
      "ğŸ•’ No improvement in logloss for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 23/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:07<00:00,  3.26it/s]\n",
      "[Epoch 23/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:29<00:00,  3.53it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0050 || Valid Loss : 0.2419 | Valid Accuracy : 94.0253%\n",
      "ğŸ•’ No improvement in logloss for 4 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 24/100] Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:03<00:00,  3.37it/s]\n",
      "[Epoch 24/100] Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:28<00:00,  3.68it/s]\n",
      "c:\\Users\\danyo\\miniconda3\\envs\\hecto\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0054 || Valid Loss : 0.2264 | Valid Accuracy : 94.0253%\n",
      "ğŸ•’ No improvement in logloss for 5 epoch(s).\n",
      "â›” Early stopping triggered at epoch 24. Best logloss: 0.1953\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BaseModel(num_classes=len(class_names)).to(device)\n",
    "best_logloss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "\n",
    "    # ë¡œê·¸ ê¸°ë¡\n",
    "    wandb.log({\n",
    "        \"Train Loss\": avg_train_loss,\n",
    "        \"Valid Loss\": avg_val_loss,\n",
    "        \"Valid Accuracy\": val_accuracy,\n",
    "        \"Validation LogLoss\": val_logloss,\n",
    "        \"LR\": optimizer.param_groups[0]['lr'],\n",
    "        \"Epoch\": epoch + 1\n",
    "    })\n",
    "\n",
    "    # ì½˜ì†” ì¶œë ¥\n",
    "    print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f} | Valid Accuracy : {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Best model ì €ì¥ + ì¡°ê¸°ì¢…ë£Œ ì²´í¬\n",
    "    if val_logloss < best_logloss:\n",
    "        best_logloss = val_logloss\n",
    "\n",
    "        # íŒŒì¼ëª… í¬ë§·: epoch_trainloss_valloss.pth\n",
    "        filename = f\"{epoch+1}_{avg_train_loss:.4f}_{avg_val_loss:.4f}.pth\"\n",
    "        save_path = os.path.join(SAVE_DIR, filename)\n",
    "\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"ğŸ“¦ Best model saved at epoch {epoch+1} ({save_path})\")\n",
    "\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"ğŸ•’ No improvement in logloss for {counter} epoch(s).\")\n",
    "        if counter >= patience:\n",
    "            print(f\"â›” Early stopping triggered at epoch {epoch+1}. Best logloss: {best_logloss:.4f}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomImageDataset(test_root, transform=val_transform, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\n",
    "model = BaseModel(num_classes=len(class_names))\n",
    "model.load_state_dict(torch.load('./checkpoints/Baseline/19_0.0009_0.1949.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# ì¶”ë¡ \n",
    "model.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # ê° ë°°ì¹˜ì˜ í™•ë¥ ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "        for prob in probs.cpu():  # prob: (num_classes,)\n",
    "            result = {\n",
    "                class_names[i]: prob[i].item()\n",
    "                for i in range(len(class_names))\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "pred = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 'ID' ì»¬ëŸ¼ì„ ì œì™¸í•œ í´ë˜ìŠ¤ ì»¬ëŸ¼ ì •ë ¬\n",
    "class_columns = submission.columns[1:]\n",
    "pred = pred[class_columns]\n",
    "\n",
    "submission[class_columns] = pred.values\n",
    "submission.to_csv('baseline_submission.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hecto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
